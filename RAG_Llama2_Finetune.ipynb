{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"8MbwDTbRHdxE"},"outputs":[],"source":["!pip install -q transformers accelerate bitsandbytes\n","!pip -q install langchain pypdf chromadb sentence-transformers faiss-gpu rank_bm25\n","!pip install datasets\n","!pip install jq"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C1xLiJVOqiQa"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VCf7rbA0qD-b"},"outputs":[],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,AutoConfig, pipeline\n","from langchain.llms import HuggingFacePipeline\n","\n","## 양자화 하기 위한 설정 ##\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","## Llama2 불러오고 토크나이저 불러오고 양자화 시키는 코드 ##\n","model_id = \"tilyupo/llama-2-7b-hf-trivia-ca2q\"\n","model = AutoModelForCausalLM.from_pretrained(model_id,quantization_config=bnb_config,device_map=\"auto\")\n","!huggingface-cli login\n","model_tokenizer=\"meta-llama/Llama-2-7b-chat-hf\"\n","tokenizer = AutoTokenizer.from_pretrained(model_tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LI5_QhHl7fgP"},"outputs":[],"source":["text_generation_pipeline = pipeline(\n","    model=model,\n","    tokenizer=tokenizer,\n","    task=\"text-generation\",\n","    temperature=0.2,\n","    return_full_text=True,\n","    max_new_tokens=300,\n",")\n","llm_pipeline = HuggingFacePipeline(pipeline=text_generation_pipeline)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OPn9da9UPHzG"},"outputs":[],"source":["from langchain.prompts import PromptTemplate\n","## prompt 설정해주는 코드 안넣고 싶었는데 이걸 넣어야 코드가 진행되서 넣었음\n","prompt_template = \"\"\"\n","### CONTEXT ###\n","{context}\n","\n","### QUESTION ###\n","{question}\n"," \"\"\"\n","prompt = PromptTemplate(\n","  input_variables=[\"context\", \"question\"],\n","  template=prompt_template\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5YSnZBydR4Nd"},"outputs":[],"source":["from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n","from langchain.vectorstores import FAISS\n","from langchain_community.retrievers import BM25Retriever\n","from langchain.retrievers import EnsembleRetriever\n","## 벡터 임베딩 모델 불러오고 'context'저장된 파일 text split한 후 FAISS 벡터 저장소에 임베딩 시킨 벡터 저장\n","embed_model_id=\"sentence-transformers/all-MiniLM-L6-v2\"\n","embeddings = HuggingFaceEmbeddings(model_name=embed_model_id,model_kwargs={\"device\":\"cuda\"})"]},{"cell_type":"code","source":["import re\n","import string\n","import collections\n","## EM Score / F1 Score 계산하기 위한 코드\n","def normalize_answer(s):\n","  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n","  def remove_articles(text):\n","    regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n","    return re.sub(regex, ' ', text)\n","  def white_space_fix(text):\n","    return ' '.join(text.split())\n","  def remove_punc(text):\n","    exclude = set(string.punctuation)\n","    return ''.join(ch for ch in text if ch not in exclude)\n","  def lower(text):\n","    return text.lower()\n","  return white_space_fix(remove_articles(remove_punc(lower(s))))\n","def get_tokens(s):\n","  if not s: return []\n","  return normalize_answer(s).split()\n","\n","def compute_exact(a_gold, a_pred):\n","  if a_pred == None:\n","    return 0\n","  return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n","\n","def compute_f1(a_gold, a_pred):\n","  if a_pred == -1:\n","    return -1\n","  gold_toks = get_tokens(a_gold)\n","  pred_toks = get_tokens(a_pred)\n","  common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n","  num_same = sum(common.values())\n","  if len(gold_toks) == 0 or len(pred_toks) == 0:\n","    # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n","    return int(gold_toks == pred_toks)\n","  if num_same == 0:\n","    return 0\n","  precision = 1.0 * num_same / len(pred_toks)\n","  recall = 1.0 * num_same / len(gold_toks)\n","  f1 = (2 * precision * recall) / (precision + recall)\n","  return f1\n","def find_answer_pred(result):\n","  answer_pred = -1\n","  answer_tag = \"### ANSWER ###\"\n","  start_idx = result.find(answer_tag)\n","  if start_idx == -1:\n","      print(\"ANSWER section not found.\")\n","      return answer_pred\n","  else:\n","    start_idx += len(answer_tag)\n","    end_idx = result.find(\"###\", start_idx)\n","    if end_idx == -1:\n","        answer_section = result[start_idx:].strip()\n","    else:\n","        answer_section = result[start_idx:end_idx].strip()\n","    answer_pred = answer_section.split('\\n')[0]\n","  return answer_pred"],"metadata":{"id":"BbU2Kh5zGsTJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kUuVrvDcUKOR"},"outputs":[],"source":["from langchain.schema.runnable import RunnablePassthrough\n","from datasets import load_dataset\n","import numpy as np\n","from langchain.chains import RetrievalQA\n","from langchain.docstore.document import Document\n","dataset = load_dataset('trivia_qa','unfiltered')\n","test_data = dataset['validation']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iPXWPEfcWz_U"},"outputs":[],"source":["import sys\n","import re\n","from langchain_community.retrievers import BM25Retriever\n","exact_scores=[]\n","f1_scores=0\n","i=1\n","for item in test_data:\n","    text_groups=[]\n","    question = item[\"question\"]\n","    try:\n","      context = item['search_results']['search_context'][0]\n","    except IndexError:\n","      continue\n","    answer_list = item[\"answer\"]['aliases']\n","    lines = context.split('\\n')\n","    for j in range(0,len(lines),4):\n","      text_groups.append('. '.join(lines[j:j+4]))\n","    text_groups = np.array(text_groups)\n","    doc = [Document(page_content=text, metadata={\"source\": \"local\"}) for text in text_groups]\n","    db = FAISS.from_documents(doc, embeddings)\n","    faiss_retriever = db.as_retriever(\n","                            search_type=\"similarity\",\n","                            search_kwargs={'k': 3}\n","                            )\n","\n","    qa_chain = RetrievalQA.from_chain_type(\n","        llm=llm_pipeline,\n","        chain_type=\"stuff\",\n","        retriever=faiss_retriever,\n","        return_source_documents=True,\n","        chain_type_kwargs={\"prompt\": prompt}\n","    )\n","    result = qa_chain.invoke(question)\n","    answer_pred = find_answer_pred(result['result'])\n","    f1_score=-1\n","    for answer in answer_list:\n","      score = compute_f1(answer,answer_pred)\n","      if score>f1_score:\n","        print(f\"{answer} = {answer_pred} : {score}\")\n","        f1_score=score\n","    if f1_score!=-1:\n","      i=i+1\n","      f1_scores+=f1_score\n","    else:\n","      i=i-1\n","      if i==0:\n","        i=1\n","    print(i)\n","    print(f\"정답 : {answer_list}\")\n","    print(f\"예측 : {answer_pred}\")\n","    print(f1_score)\n","    print(f1_scores/i)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0rUW7ghriVMY"},"outputs":[],"source":["from langchain.schema.runnable import RunnablePassthrough\n","from datasets import load_dataset\n","import numpy as np\n","from langchain.chains import RetrievalQA\n","from langchain.docstore.document import Document\n","dataset = load_dataset('squad',split='validation')\n","exact_scores=[]\n","f1_scores=0\n","i=1\n","\n","## squad 데이터셋 갯수 만큼 question에 맞는 answer를 생성하기 위한 for문\n","for item in dataset:\n","\n","    question = item[\"question\"]\n","    answer = item[\"answers\"][\"text\"][0]\n","    texts = item['context'].split('.')\n","    doc =  [Document(page_content=text, metadata={\"source\": \"local\"}) for text in texts]\n","    db = FAISS.from_documents(doc, embeddings)\n","    faiss_retriever = db.as_retriever(\n","                            search_type=\"similarity\",\n","                            search_kwargs={'k': 2}\n","                            )\n","    bm25_retriever = BM25Retriever.from_texts(item['context'],metadatas=[{'source': 'local'}])\n","    bm25_retriever.k = 2\n","    ensemble_retriever = EnsembleRetriever(\n","        retrievers=[bm25_retriever, faiss_retriever],weigths=[0.5,0.5]\n","    )\n","\n","    qa_chain = RetrievalQA.from_chain_type(\n","        llm=llm_pipeline,\n","        chain_type=\"stuff\",\n","        retriever=faiss_retriever,\n","        return_source_documents=True,\n","        chain_type_kwargs={\"prompt\": prompt}\n","    )\n","\n","    result = qa_chain.invoke(question)\n","    ## prompt 설정한거 전부 버리고 정답만을 출력하기 위한 코드 -> prompt에 맞게 출력한것을 보고 싶으면 >> print(result)\n","    answer_pred = find_answer_pred(result['result'])\n","\n","    ## EM Score/F1 Score 계산\n","    exact_score = compute_exact(answer,answer_pred)\n","    f1_score = compute_f1(answer,answer_pred)\n","    if f1_score!=-1:\n","      f1_scores+=f1_score\n","    else:\n","      i=i-1\n","      if i==0:\n","        i=1\n","\n","    print(i)\n","    print(f\"정답 : {answer}\")\n","    print(f\"예측 : {answer_pred}\")\n","    print(f1_score)\n","    print(f1_scores/i)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UcZZergU9qri"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1mHjbjCjlCgojB59sxdbUg5qy2RFV3ktC","timestamp":1712996813907}],"mount_file_id":"15owmNggFvTTepkEChzljebNPFi_i-9Od","authorship_tag":"ABX9TyPJChs3sRi1RASuOYiznA8x"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}